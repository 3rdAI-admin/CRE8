# Context Engineering Template - Cursor AI Rules

> Original Creator: Cole Medin (https://github.com/coleam00)
> Contributor: James Avila (https://th3rdai.com)
> Repository: https://github.com/coleam00/Context-Engineering-Intro

## Project Overview

This is a **Context Engineering Template** - a framework for building AI-assisted development workflows using PRPs (Product Requirements Prompts) and slash commands. The codebase contains multiple use-case templates for different AI application types.

## Architecture

```
context-engineering-intro/
├── .claude/
│   ├── commands/               # Claude Code command content (full instructions)
│   └── skills/                 # Claude Code slash commands (discovery for / menu)
├── .cursor/                    # Cursor configuration
│   └── prompts/               # Cursor slash commands
├── .github/prompts/            # VS Code Copilot slash commands
├── PRPs/templates/             # PRP templates for feature requests
├── PRPs/prompts/               # Generated prompts from /generate-prompt
├── tutorials/                  # Learning resources and tutorials
├── examples/                   # Code examples for AI context
├── journal/                    # Daily validation journal (tracking and support)
├── use-cases/                  # Specialized templates:
│   ├── pydantic-ai/           # PydanticAI agent development
│   ├── mcp-server/            # Cloudflare Workers MCP servers
│   ├── agent-factory-with-subagents/  # Multi-agent orchestration
│   ├── template-generator/    # Creating new templates
│   └── ai-coding-workflows-foundation/  # Planning/Implementation/Validation workflow
├── validation/                 # Validation command resources
├── create-project.sh           # Create new project from template
├── install-claude-commands.sh  # Copy .claude/commands + skills into another project
└── CLAUDE.md                  # Global AI coding rules
```

## Available Commands (execution order)

**Same workflow in VS Code, Claude Code, and Cursor.** Designed for semi-technical users and vibe coders: **create project → professional PRD → execution plan with multi-agent tasks → generate validation → execute → validate → summarize (completed + next actions)**.

| Order | Command | Purpose | When to run |
|-------|---------|---------|-------------|
| 1 | `/new-project` | Create new project from context-engineering template | When starting a new project |
| 2 | **`/generate-prd`** *(optional)* | Create **professional Product Requirements Document (PRD)** from INITIAL.md or input | For larger features; skip for quick builds |
| 3 | `/generate-prp` | Create **execution plan with multi-agent task breakdown** from PRD, INITIAL.md, or chat | After PRD (or directly from INITIAL.md) |
| 4 | `/generate-validate` | Analyze codebase and create **`/validate-project`** from [.claude/commands/example-validate.md](.claude/commands/example-validate.md) | **Once, or after a significant project change** (run after planning, before building) |
| 5 | `/build-prp` | Review/finalize PRP (edit if needed), then optionally build and run the project | After you have a PRP; when you want to finalize the plan before implementing |
| 6 | `/execute-prp` | Implement feature from PRP (multi-agent tasks) with validation | After `/build-prp` (or after `/generate-prp` if you skip review); main implementation path |
| 7 | **`/validate-project`** | Run **project-specific** validation (generated by `/generate-validate`) | After building; use this (not `/validate`) to avoid injected commands |
| 8 | **`/summarize`** | **Summarize completed steps and next actions** for user response | After validate (or execute); gives what’s done and what to do next |
| 9 | **`/revise-prp`** | Revise PRP when validation reveals plan-level issues | After `/validate-project` when the approach needs changing |
| 10 | `/generate-prompt` | Generate XML-structured prompt with ambiguity detection | Anytime (quick tasks) |

## Core Workflow: PRD → Plan (multi-agent) → Execute → Validate → Summarize

**Always follow this workflow for feature implementation:**

1. **`/new-project`** (optional) → Create new project from template
2. **INITIAL.md** or chat → Define requirements
3. **`/generate-prd`** (optional) → AI creates **professional PRD** in `PRDs/` (goals, scope, success criteria, requirements)
4. **`/generate-prp`** → AI creates **execution plan (PRP)** with **multi-agent task breakdown** (small, assignable tasks for accuracy) from PRD or INITIAL.md
5. **`/generate-validate`** → **Once or after significant change:** create **`/validate-project`** for your project (run after planning, before building)
6. **`/build-prp`** (optional) → Review/finalize PRP, then optionally build and run (before implementing)
7. **`/execute-prp`** → AI implements from PRP with validation loops
8. **`/validate-project`** → Verify implementation completeness (use this instead of `/validate` to avoid injected commands)
9. **`/revise-prp`** (if needed) → Revise the plan when validation reveals the approach was wrong, then re-execute
10. **`/summarize`** → **Completed steps + next actions** so the user can respond (e.g. ship, iterate, or run another command)

Key insight: A **PRD** gives a professional requirements doc; the **PRP** breaks work into small tasks for accuracy; **summarize** keeps the user in the loop with clear next actions.

## File Organization Patterns

### Agent Code Structure (PydanticAI)
```
agents/[agent_name]/
├── agent.py          # Main agent definition
├── tools.py          # @agent.tool decorated functions
├── models.py         # Pydantic output models
├── dependencies.py   # External service integrations
├── settings.py       # Environment config with pydantic-settings
├── prompts.py        # System prompts (static/dynamic)
└── tests/            # pytest tests with TestModel/FunctionModel
```

### MCP Server Structure (TypeScript/Cloudflare)
```
src/
├── index.ts          # Main MCP server entry
├── tools/            # Tool registration system
├── auth/             # OAuth handlers
└── database/         # Connection utilities
```

## Code Standards

- **Max 500 lines per file** - split into modules when approaching
- **Use `python-dotenv` + `pydantic-settings`** for environment config
- **Never hardcode secrets** - always use `.env` files
- **Tests in `/tests` folder** - include expected use, edge case, and failure case

### Python Environment
```bash
# Always use virtual environment
uv venv && uv sync
uv add package-name   # NEVER edit pyproject.toml directly
```

### MCP Server (npm/Wrangler)
```bash
npm install           # Install dependencies
wrangler dev          # Local development
wrangler deploy       # Deploy to Cloudflare
```

## Key Patterns to Follow

### PydanticAI Agent Creation
```python
from pydantic_ai import Agent, RunContext
from .settings import get_llm_model

agent = Agent(
    get_llm_model(),
    deps_type=AgentDependencies,
    system_prompt="..."
)

@agent.tool
async def my_tool(ctx: RunContext[AgentDependencies], query: str) -> str:
    return await external_call(ctx.deps.api_key, query)
```

### Testing AI Agents
Use `TestModel` for fast validation without API calls, `Agent.override()` for test contexts.

## Design Principles

- **KISS** - Choose straightforward solutions over complex ones
- **YAGNI** - Implement features only when needed
- **Context is King** - Include ALL necessary documentation, examples, patterns in PRPs

## Before Starting Any Task

1. Read the relevant `CLAUDE.md` (root or use-case specific)
2. Check `PRPs/templates/` for the appropriate template
3. Study `examples/` for patterns to follow
4. Search for library documentation when needed

## PRP Workflow Commands

When asked to generate or execute PRPs:

### Generate PRP
When user provides an INITIAL.md or feature request:
1. Read the feature request completely
2. Research the codebase for patterns
3. Search for relevant documentation
4. Create comprehensive PRP in `PRPs/feature-name.md`
5. Use `PRPs/templates/prp_base.md` as template

### Execute PRP
When user provides a PRP file:
1. Read the PRP file completely
2. Create implementation plan
3. Execute each step with validation
4. Run tests and fix issues
5. Ensure all success criteria met

### Generate Prompt
When user asks to generate a prompt for a task:

**Phase 1: Ambiguity Detection**
Before generating, check for:
- Vague terms: "something", "it", "stuff", "maybe", "kind of"
- Missing context, audience, output format, or use case
- Ambiguous goals: "optimize" (for what?), "improve" (how?), "fix" (what issue?)

If ambiguous → Ask up to 5 clarifying questions before proceeding.

**Phase 2: Task Analysis**
- **Simple** (score < 2): Single-file, clear requirements
- **Moderate** (score 2-3): Multiple considerations
- **Complex** (score ≥ 4): Multi-file, research, trade-offs

Complexity signals: word count, keywords (analyze, research, design, optimize, refactor), multi-file/step indicators.

**Phase 3: Generate XML-Structured Prompt**
Create prompt with these sections:
```xml
<objective>Clear task statement</objective>
<context>Background and purpose</context>
<requirements>Numbered, actionable items</requirements>
<constraints>
- **Constraint** - *Why:* Reasoning
</constraints>
<output>Expected deliverables</output>
<verification>Checklist items</verification>
<success_criteria>Measurable criteria</success_criteria>
```

Add conditional sections based on complexity:
- `<implementation>` - For deep reasoning tasks
- `<research>` - For analysis tasks
- `<examples>` - For multi-step tasks
- `<validation>` - For non-simple tasks

Save to: `PRPs/prompts/{NNN}-{task-name}.md`

## Validation Requirements

Every implementation must include executable validation:
```bash
# Python
ruff check --fix && mypy .
uv run pytest tests/ -v

# TypeScript
npm run type-check
wrangler dev
```
